{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-NER-Test-01-07.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Fw3yWTvZzo8"
      },
      "source": [
        "# Reconocimiento de Entidad por Nombre\r\n",
        "Codigo de Estudiante:....\r\n",
        "\r\n",
        "Apellidos y Nombres:.....\r\n",
        "\r\n",
        "Fecha de Presentación:...\r\n",
        "\r\n",
        "@author: mfch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CzFe-eiZ7uz"
      },
      "source": [
        "## Test 01\r\n",
        "\r\n",
        "**Instrucciones**\r\n",
        "1. Cargue el archivo *uber_apple.txt* en la variable **article**\r\n",
        "2. Tokenice **article** en oraciones o sentences con **nltk.sent_tokenize**.\r\n",
        "3. Tokenice cada oración en *sentences* usando una lista comprimida y **nltk.word_token**.\r\n",
        "4. Dentro de una lista comprimida, etiquete cada oración tokenizada en *partes del discurso* usando **nltk.pos_tag()**.\r\n",
        "5. Divida cada oración etiquetada en trozos de entidad con nombre usando **nltk.ne_chunk_sents()**. Junto con *pos_sentences*, especifique el argumento de palabra clave adicional **binary = True**.\r\n",
        "5. Itere sobre cada *sentence* y cada *chunk*, y compruebe si se trata de un *chunk* de entidad con nombre al probar si tiene la etiqueta **label** y si **chunk.label()** es igual a **\"NE\"**. Si es así, imprima ese *chunk*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EinzvZ8PZTKF"
      },
      "source": [
        "with open('/content/uber_apple.txt', mode='r', encoding='utf-8-sig') as f:\r\n",
        "  article = f.read()\r\n",
        "article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myKRJu2LadGe"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "nltk.download('maxent_ne_chunker')\r\n",
        "nltk.download('words')\r\n",
        "\r\n",
        "# Tokenize the article into sentences: sentences\r\n",
        "sentences = ____\r\n",
        "\r\n",
        "# Tokenize each sentence into words: token_sentences\r\n",
        "token_sentences = [____ for sent in ____]\r\n",
        "\r\n",
        "# Tag each tokenized sentence into parts of speech: pos_sentences\r\n",
        "pos_sentences = [____ for sent in ____] \r\n",
        "\r\n",
        "# Create the named entity chunks: chunked_sentences\r\n",
        "chunked_sentences = ____\r\n",
        "\r\n",
        "# Test for stems of the tree with 'NE' tags\r\n",
        "for sent in chunked_sentences:\r\n",
        "    for chunk in sent:\r\n",
        "        if hasattr(chunk, \"label\") and ____ == \"____\":\r\n",
        "            print(chunk)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNp1erwGh4-h"
      },
      "source": [
        "## Test 02\r\n",
        "\r\n",
        "**Instrucciones**\r\n",
        "1. Cree un diccionario *defaultdict* llamado **ner_categories**, con el tipo por defecto configurado a **int**\r\n",
        "2. Llene el diccionario con valores para cada una de las claves **keys**. Recuerde, las claves representaran la etiqueta label(). En el *for* de más afuera itere sobre **chunked_sentences**, usando **sent** como la variable de iteración. En el *for* interno, itere sobre **sent**. Si la condición es verdadera, incremente el valor de cada clave **key** en 1. *Recuerde usar label() por cada chunk*.\r\n",
        "3. Para las etiquetas de la torta, cree una lista llamada **labels** a partir de las claves de **ner_categories**, que puede ser accedido usando**.keys()**.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKrW0tHdNExN"
      },
      "source": [
        "from collections import defaultdict\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "chunked_sentences = nltk.ne_chunk_sents(pos_sentences)\r\n",
        "\r\n",
        "# Create the defaultdict: ner_categories\r\n",
        "ner_categories = _____\r\n",
        "\r\n",
        "# Create the nested for loop\r\n",
        "for ____ in ____:\r\n",
        "    for chunk in ____:\r\n",
        "        if hasattr(chunk, 'label'):\r\n",
        "            ____[____.____] += 1\r\n",
        "            \r\n",
        "# Create a list from the dictionary keys for the chart labels: labels\r\n",
        "labels = list(_____)\r\n",
        "\r\n",
        "# Create a list of the values: values\r\n",
        "values = [____.____ for v in labels]\r\n",
        "\r\n",
        "# Create the pie chart\r\n",
        "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\r\n",
        "\r\n",
        "# Display the chart\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-F7TmmOorWE"
      },
      "source": [
        "## Test 04\r\n",
        "\r\n",
        "**Instrucciones**\r\n",
        "1. Importe spacy.\r\n",
        "2. Cargue el modelo en inglés 'en' usando spacy.load(). Especifique los argumentos adicionales **tagger=False, parser=False, matcher=False**.\r\n",
        "3. Cree un objeto *document* **doc** de spacy pasando **article** a  **nlp()**.\r\n",
        "4. Use **ent** como la variable de iteración, itere sobre las **entities** de **doc** e imprima las etiquetas **ent.label_** y el  texto **ent.text**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpeXFYbkouBG"
      },
      "source": [
        "# Import spacy\r\n",
        "____\r\n",
        "\r\n",
        "# Instantiate the English model: nlp\r\n",
        "nlp = ____\r\n",
        "\r\n",
        "# Create a new document: doc\r\n",
        "doc = ____\r\n",
        "\r\n",
        "# Print all of the found entities and their labels\r\n",
        "for ____ in ____:\r\n",
        "    print(____, ____)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPCjrvsraCHo"
      },
      "source": [
        "## Test 06\r\n",
        "\r\n",
        "**Instrucciones**\r\n",
        "1. Instale polyglot, pyicu, Morfessur, pycld2\r\n",
        "2. Descargue ner2.fr y embbedings.fr; necesarios para usar francés\r\n",
        "3. Importe la clase **Text** desde polyglot.text\r\n",
        "4. Usando la cadena en **article** , cree un nuevo objeto **Text** llamado **txt**.\r\n",
        "5. Itere sobre **txt.entities** e imprimar cada entidad **ent**.\r\n",
        "6. Imprima el tipo de ent con **type()**.\r\n",
        "7. Use una lista comprimida para crear una lista de tuplas llamada **entities**. Las expresión de salida de la lista debe ser una tupla, donde el Primer elemento es la etiqueta de la entidad (use el atributo **.tag** de la entidad **ent**); y el segundo elemento es la cadena completa del texto de la entidad (use **.join(ent)**). La variable de iteración debe ser **ent** para iterar sobre **entities** del objeto Text de polyglot llamado **txt**.\r\n",
        "8. Imprima **entities** para ver lo que se ha creado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5DohzKBdJ7_"
      },
      "source": [
        "!pip3 install polyglot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G54uKRYqezYt"
      },
      "source": [
        "!pip3 install pyicu "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj-HiATnfCTQ"
      },
      "source": [
        "!pip3 install Morfessor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E2w4NmrfHqK"
      },
      "source": [
        "!pip3 install pycld2 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtBzpvv7d62k"
      },
      "source": [
        "%%bash\r\n",
        "polyglot download ner2.fr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlVnprhqfhaT"
      },
      "source": [
        "%%bash\r\n",
        "polyglot download embeddings2.fr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FpQ7xCTaIib"
      },
      "source": [
        "with open('/content/french.txt', mode='r', encoding='utf-8-sig') as f:\r\n",
        "  article = f.read()\r\n",
        "\r\n",
        "# Importe la clase Text desde polyglot.text\r\n",
        "from polyglot.text import ____\r\n",
        "\r\n",
        "# Create a new text object using Polyglot's Text class: txt\r\n",
        "txt = ____\r\n",
        "\r\n",
        "# Print each of the entities found\r\n",
        "for ____ in ____:\r\n",
        "    ____\r\n",
        "    \r\n",
        "# Print the type of ent\r\n",
        "____\r\n",
        "\r\n",
        "# Create the list of tuples: entities\r\n",
        "entities = [(____.____, ' '.____(____)) for ____ in ____.____]\r\n",
        "\r\n",
        "# Print entities\r\n",
        "____\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcPs_-nn7z2l"
      },
      "source": [
        "## Test 07\r\n",
        "\r\n",
        "**Instrucciones**\r\n",
        "1. Itere sobre todas las **entities** de txt, usando **ent** como la variable de iteración.\r\n",
        "2. Verifique si la entidad contiene *\"Márquez\"* o *\"Gabo\"*. Si es así, incremente el contador **count**. No olvide incluir el acento a \"Márquez\".\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjeJjELm_a8W"
      },
      "source": [
        "%%bash\r\n",
        "polyglot download embeddings2.es ner2.es"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZN-jExwAm8i"
      },
      "source": [
        "with open('/content/marquez.txt', mode='r', encoding='utf-8-sig') as f:\r\n",
        "  article = f.read()\r\n",
        "\r\n",
        "article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJrbj_oi96bU"
      },
      "source": [
        "from polyglot.text import Text\r\n",
        "\r\n",
        "# Create a new text object using Polyglot's Text class: txt\r\n",
        "txt = Text(article)\r\n",
        "# Initialize the count variable: count\r\n",
        "count = 0\r\n",
        "\r\n",
        "# Iterate over all the entities\r\n",
        "____\r\n",
        "    # Check whether the entity contains 'Márquez' or 'Gabo'\r\n",
        "    ____\r\n",
        "        # Increment count\r\n",
        "        ____\r\n",
        "\r\n",
        "# Print count\r\n",
        "print(\"Entidades Encontradas sobre Márquez o Gabo: \",count)\r\n",
        "\r\n",
        "# Calculate the percentage of entities that refer to \"Gabo\": percentage\r\n",
        "percentage = count / len(txt.entities)\r\n",
        "print(\"Porcentaje de las Entidades Máquez o Gabo: \",percentage)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}